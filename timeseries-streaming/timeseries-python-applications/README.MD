# Quickstart

----

*** Note ***

This samples intent is to explore the data engineering needed to work with data generated from a streaming Beam 
pipeline and delivered to an auto encoder - decoder.

It is not intended to demonstrate cutting edge ML model techniques. 

The sample data is very simple repeating pattern, effectively making any train / eval / test void as the data will 
simply repeat across all samples.  

In these samples we are using FIRST and LAST in the learning, and the outlier test, this is for demonstration purposes 
but is not correct given the library aggregates all data using the type 1 computations. The artificial data used happens 
to have two values per 'tick' so this works ok here. In future samples we will make use of MAX / MIN, and the other 
aggregations with more complex data.   

TODO explain outliers, bootstrapping and filling gaps with data examples

----

## Using TF.Example's using a Tensor Flow Extended (TFX) pipeline

* Setup virtual environment
```
virtualenv -p python3.7 streaming-tf-consumer
source streaming-tf-consumer/bin/activate
```

```
git clone https://github.com/GoogleCloudPlatform/dataflow-sample-applications.git
cd dataflow-sample-applications/timeseries-streaming/timeseries-python-applications
cd MLPipeline
pip install -e .
cd ../MLPipelineExamples/test_pipelines
ls
```

From here there are two options: 

* Option 1 : Use a prebuilt [saved_model_example](MLPipelineExamples/test_pipelines/saved_model_example) to run the inference on.
* Option 2 : Generate bootstrap data to train the model on.

# Option 1

There is a pre-built model available in the folder [saved_model_example](MLPipelineExamples/test_pipelines/saved_model_example). The model, was built using data generated by [SimpleDataBootstrapGenerator.java](timeseries-java-applications/SyntheticExamples/src/main/java/com/google/dataflow/sample/timeseriesflow/examples/simpledata/transforms/SimpleDataBootstrapGenerator.java).
The data is always repeating, which is a useful property for us to explore the data engineering around the use of the streaming data with TFX. In order to use the model we make use of [RunInferene](https://github.com/tensorflow/tfx-bsl/blob/master/tfx_bsl/beam/run_inference.py) tfx-bsl. 

There are two versions which can be run;

[batch_inference](MLPipelineExamples/test_pipelines/batch_inference.py)
* Step 1 - Follow the quick start on the Java README to run [SimpleDataStreamGenerator.java](../timeseries-java-applications/SyntheticExamples/src/main/java/com/google/dataflow/sample/timeseriesflow/examples/simpledata/transforms/SimpleDataStreamGenerator.java) with example_2.
* Step 2 - Run the command with the virtual-env activated, providing values for the location of the ```--saved_model_location``` using the uncompressed provided model, and the location of the generated data from previous step with ```--tfrecord_folder```.
```
python MLPipelineExamples/test_pipelines/batch_inference.py --saved_model_location=MLPipelineExamples/test_pipelines/saved_model_example/serving_model_dir --tfrecord_folder=/<your-directory>/simple-data/data/*
```
Dependent on how long you ran the ```SimpleDataStreamGenerator``` you will see outliers in the range of 120 to 300 being detected.

[stream_inference](MLPipelineExamples/test_pipelines/stream_inference.py)
* Step 1 - Follow the quick start on the Java README to run [SimpleDataStreamGenerator.java](../timeseries-java-applications/SyntheticExamples/src/main/java/com/google/dataflow/sample/timeseriesflow/examples/simpledata/transforms/SimpleDataStreamGenerator.java) with example_4.
* Step 2 - Create a subscription in the topic, to the read events generated in previous step
* Step 3 - Run the command with the virtual-env activated, providing values for the location of the ```--saved_model_location``` using the uncompressed provided model, and set the subscription to read events from ```--pubsub_subscription```.
```
python MLPipelineExamples/test_pipelines/stream_inference.py --saved_model_location=MLPipelineExamples/test_pipelines/saved_model_example/serving_model_dir --pubsub_subscription=projects/<your-project>/subscriptions/<your-subscription>
``` 

While both are running you will see outliers showing every 50 ticks.

# Option 2

In order to build the model you will need to first run the generator job [SimpleDataBootstrapGenerator.java](../timeseries-java-applications/SyntheticExamples/src/main/java/com/google/dataflow/sample/timeseriesflow/examples/simpledata/transforms/SimpleDataBootstrapGenerator.java).
That job will generate 86400 data points, which will be aggregated to 43200 type 1 computations. ( It is recommended you run that job on a production runner like Dataflow rather than the DirectRunner.)

Once that is done, change the information in the [config.py](MLPipelineExamples/test_pipelines/config.py) to match your local env.
Run the command with the virtual-env activated:
```
python MLPipelineExamples/test_pipelines/timeseries_local_simple_data.py
``` 
You should see the model building as below.

```
....
Epoch 24/30
280/280 [==============================] - 4s 16ms/step - loss: 3.0109 - mean_absolute_error: 1.3696 - val_loss: 2.1298 - val_mean_absolute_error: 0.9847
Epoch 25/30
280/280 [==============================] - 5s 17ms/step - loss: 2.9315 - mean_absolute_error: 1.3585 - val_loss: 3.3036 - val_mean_absolute_error: 1.3569
Epoch 26/30
280/280 [==============================] - 5s 17ms/step - loss: 2.9117 - mean_absolute_error: 1.3525 - val_loss: 1.6720 - val_mean_absolute_error: 0.9148
Epoch 27/30
280/280 [==============================] - 5s 18ms/step - loss: 2.7094 - mean_absolute_error: 1.3105 - val_loss: 1.8781 - val_mean_absolute_error: 1.0250
Epoch 28/30
280/280 [==============================] - 5s 17ms/step - loss: 2.7709 - mean_absolute_error: 1.3234 - val_loss: 3.1052 - val_mean_absolute_error: 1.3147
Epoch 29/30
280/280 [==============================] - 5s 18ms/step - loss: 2.7719 - mean_absolute_error: 1.3296 - val_loss: 1.4818 - val_mean_absolute_error: 0.9848
Epoch 30/30
280/280 [==============================] - 5s 18ms/step - loss: 2.6756 - mean_absolute_error: 1.3105 - val_loss: 1.6916 - val_mean_absolute_error: 0.9970
```

This will output a serving_model_dir under the location you specified for ```PIPELINE_ROOT``` in the config.py file. With this you can now follow the rest of the steps outlines in Option 1 but using your own model.
For example:
```
python MLPipelineExamples/test_pipelines/batch_inference.py --saved_model_location=<PIPELINE_ROOT>/serving_model_dir --tfrecord_folder=/<your-directory>/simple-data/data/*
```
